{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed08d28-987a-4b45-95af-f4fe193e7d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_params': {'im_train_path': '/Users/narayanmurti/Workspace/Dissertation/data/citypersons_dir/train/images', 'ann_train_path': '/Users/narayanmurti/Workspace/Dissertation/data/citypersons_dir/train/train_df.csv', 'im_test_path': '/Users/narayanmurti/Workspace/Dissertation/data/citypersons_dir/valid/images', 'ann_test_path': '/Users/narayanmurti/Workspace/Dissertation/data/citypersons_dir/valid/val_df.csv', 'num_classes': 1}, 'model_params': {'im_channels': 3, 'aspect_ratios': [0.5, 1, 2], 'scales': [128, 256, 512], 'min_im_size': 600, 'max_im_size': 1000, 'backbone_out_channels': 512, 'fc_inner_dim': 1024, 'rpn_bg_threshold': 0.3, 'rpn_fg_threshold': 0.7, 'rpn_nms_threshold': 0.7, 'rpn_train_prenms_topk': 12000, 'rpn_test_prenms_topk': 6000, 'rpn_train_topk': 2000, 'rpn_test_topk': 300, 'rpn_batch_size': 256, 'rpn_pos_fraction': 0.5, 'roi_iou_threshold': 0.5, 'roi_low_bg_iou': 0.0, 'roi_pool_size': 7, 'roi_nms_threshold': 0.3, 'roi_topk_detections': 100, 'roi_score_threshold': 0.05, 'roi_batch_size': 128, 'roi_pos_fraction': 0.25}, 'train_params': {'task_name': 'frcnn_citypersons', 'seed': 1111, 'acc_steps': 1, 'num_epochs': 20, 'lr_steps': [12, 16], 'lr': 0.001, 'ckpt_name': 'frcnn_citypersons.pth'}}\n",
      "{0: 'background', 1: 'person'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2500it [01:45, 23.71it/s]\n",
      "/Users/narayanmurti/opt/anaconda3/envs/pytorvch_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/narayanmurti/opt/anaconda3/envs/pytorvch_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 2500 images found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/625 [00:00<?, ?it/s][W NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "  0%|                                         | 1/625 [00:25<4:29:26, 25.91s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "All bounding boxes should have positive height and width. Found invalid box [4.8828125, 208.0078125, 13.671875, 26.3671875] for target at index 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 153\u001b[0m\n\u001b[1;32m    150\u001b[0m     use_resnet50_fpn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# or False\u001b[39;00m\n\u001b[1;32m    152\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[0;32m--> 153\u001b[0m train(args)\n",
      "Cell \u001b[0;32mIn[3], line 105\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    103\u001b[0m     target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    104\u001b[0m images \u001b[38;5;241m=\u001b[39m [im\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m ims]\n\u001b[0;32m--> 105\u001b[0m batch_losses \u001b[38;5;241m=\u001b[39m faster_rcnn_model(images, targets)\n\u001b[1;32m    106\u001b[0m loss \u001b[38;5;241m=\u001b[39m batch_losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_box_reg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorvch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorvch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorvch_env/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py:95\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     93\u001b[0m             bb_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(degenerate_boxes\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     94\u001b[0m             degen_bb: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes[bb_idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 95\u001b[0m             torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[1;32m     96\u001b[0m                 \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll bounding boxes should have positive height and width.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found invalid box \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegen_bb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for target at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m             )\n\u001b[1;32m    101\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(images\u001b[38;5;241m.\u001b[39mtensors)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorvch_env/lib/python3.12/site-packages/torch/__init__.py:1559\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m-> 1559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[0;31mAssertionError\u001b[0m: All bounding boxes should have positive height and width. Found invalid box [4.8828125, 208.0078125, 13.671875, 26.3671875] for target at index 1."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import yaml\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "\n",
    "# move into the root directory to find my data module\n",
    "os.chdir('/Users/narayanmurti/Workspace/Dissertation')\n",
    "sys.path.append(os.getcwd())\n",
    "from data.citypersons import CitypersonsDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def collate_function(data):\n",
    "    return tuple(zip(*data))\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # Read the config file #\n",
    "    with open(args.config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    print(config)\n",
    "    ########################\n",
    "\n",
    "    dataset_config = config['dataset_params']\n",
    "    train_config = config['train_params']\n",
    "\n",
    "    seed = train_config['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    citypersons = CitypersonsDataset(split = 'train',\n",
    "                     im_dir=dataset_config['im_train_path'],\n",
    "                     ann_file=dataset_config['ann_train_path'])\n",
    "\n",
    "    train_dataset = DataLoader(citypersons,\n",
    "                               batch_size=4,\n",
    "                               shuffle=True,\n",
    "                               num_workers=4,\n",
    "                               collate_fn=collate_function)\n",
    "\n",
    "    if args.use_resnet50_fpn:\n",
    "        faster_rcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True,\n",
    "                                                                                 min_size=600,\n",
    "                                                                                 max_size=1000,\n",
    "        )\n",
    "        faster_rcnn_model.roi_heads.box_predictor = FastRCNNPredictor(\n",
    "            faster_rcnn_model.roi_heads.box_predictor.cls_score.in_features,\n",
    "            num_classes=21)\n",
    "    else:\n",
    "        backbone = torchvision.models.resnet34(pretrained=True, norm_layer=torchvision.ops.FrozenBatchNorm2d)\n",
    "        backbone = torch.nn.Sequential(*list(backbone.children())[:-3])\n",
    "        backbone.out_channels = 256\n",
    "        roi_align = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "        rpn_anchor_generator = AnchorGenerator()\n",
    "        faster_rcnn_model = torchvision.models.detection.FasterRCNN(backbone,\n",
    "                                                                    num_classes=21,\n",
    "                                                                    min_size=600,\n",
    "                                                                    max_size=1000,\n",
    "                                                                    rpn_anchor_generator=rpn_anchor_generator,\n",
    "                                                                    rpn_pre_nms_top_n_train=12000,\n",
    "                                                                    rpn_pre_nms_top_n_test=6000,\n",
    "                                                                    box_batch_size_per_image=128,\n",
    "                                                                    rpn_post_nms_top_n_test=300\n",
    "                                                                    )\n",
    "\n",
    "    faster_rcnn_model.train()\n",
    "    faster_rcnn_model.to(device)\n",
    "    if not os.path.exists(train_config['task_name']):\n",
    "        os.mkdir(train_config['task_name'])\n",
    "\n",
    "    optimizer = torch.optim.SGD(lr=1E-4,\n",
    "                                params=filter(lambda p: p.requires_grad, faster_rcnn_model.parameters()),\n",
    "                                weight_decay=5E-5, momentum=0.9)\n",
    "\n",
    "    num_epochs = train_config['num_epochs']\n",
    "    step_count = 0\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        rpn_classification_losses = []\n",
    "        rpn_localization_losses = []\n",
    "        frcnn_classification_losses = []\n",
    "        frcnn_localization_losses = []\n",
    "        for ims, targets, _ in tqdm(train_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            for target in targets:\n",
    "                target['boxes'] = target['bboxes'].float().to(device)\n",
    "                del target['bboxes']\n",
    "                target['labels'] = target['labels'].long().to(device)\n",
    "            images = [im.float().to(device) for im in ims]\n",
    "            batch_losses = faster_rcnn_model(images, targets)\n",
    "            loss = batch_losses['loss_classifier']\n",
    "            loss += batch_losses['loss_box_reg']\n",
    "            loss += batch_losses['loss_rpn_box_reg']\n",
    "            loss += batch_losses['loss_objectness']\n",
    "\n",
    "            rpn_classification_losses.append(batch_losses['loss_objectness'].item())\n",
    "            rpn_localization_losses.append(batch_losses['loss_rpn_box_reg'].item())\n",
    "            frcnn_classification_losses.append(batch_losses['loss_classifier'].item())\n",
    "            frcnn_localization_losses.append(batch_losses['loss_box_reg'].item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step_count +=1\n",
    "        print('Finished epoch {}'.format(i))\n",
    "        if args.use_resnet50_fpn:\n",
    "            torch.save(faster_rcnn_model.state_dict(), os.path.join(train_config['task_name'],\n",
    "                                                                    'tv_frcnn_r50fpn_' + train_config['ckpt_name']))\n",
    "        else:\n",
    "            torch.save(faster_rcnn_model.state_dict(), os.path.join(train_config['task_name'],\n",
    "                                                                    'tv_frcnn_' + train_config['ckpt_name']))\n",
    "        loss_output = ''\n",
    "        loss_output += 'RPN Classification Loss : {:.4f}'.format(np.mean(rpn_classification_losses))\n",
    "        loss_output += ' | RPN Localization Loss : {:.4f}'.format(np.mean(rpn_localization_losses))\n",
    "        loss_output += ' | FRCNN Classification Loss : {:.4f}'.format(np.mean(frcnn_classification_losses))\n",
    "        loss_output += ' | FRCNN Localization Loss : {:.4f}'.format(np.mean(frcnn_localization_losses))\n",
    "        print(loss_output)\n",
    "    print('Done Training...')\n",
    "\n",
    "\n",
    "#THIS IS FOR RUNNING ON THE COMMAND LINE\n",
    "#if __name__ == '__main__':\n",
    "#    parser = argparse.ArgumentParser(description='Arguments for faster rcnn using torchvision code training')\n",
    "#    parser.add_argument('--config', dest='config_path',\n",
    "#                        default='config/citypersons.yaml', type=str)\n",
    "#    parser.add_argument('--use_resnet50_fpn', dest='use_resnet50_fpn',\n",
    "#                        default=True, type=bool)\n",
    "#    args = parser.parse_args(args=[] if sys.argv[0].endswith('ipykernel_launcher.py') else sys.argv[1:])\n",
    "#    train(args)\n",
    "\n",
    "\n",
    "#THIS IS FOR RUNNING IN JUPYTER\n",
    "# Manually define the arguments instead of using argparse\n",
    "class Args:\n",
    "    config_path = 'config/citypersons.yaml'\n",
    "    use_resnet50_fpn = True  # or False\n",
    "\n",
    "args = Args()\n",
    "train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e36f0d-ab71-4dea-b0ee-6d8f97200f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorvch_env",
   "language": "python",
   "name": "pytorvch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
