{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f7007-3aa0-45b1-b978-6a5411d1ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_iou(boxes1, boxes2):\n",
    "    r\"\"\"\n",
    "    IOU between two sets of boxes\n",
    "    :param boxes1: (Tensor of shape N x 4)\n",
    "    :param boxes2: (Tensor of shape M x 4)\n",
    "    :return: IOU matrix of shape N x M\n",
    "    \"\"\"\n",
    "    # Area of boxes (x2-x1)*(y2-y1)\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # (N,)\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # (M,)\n",
    "    \n",
    "    # Get top left x1,y1 coordinate\n",
    "    x_left = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # (N, M)\n",
    "    y_top = torch.max(boxes1[:, None, 1], boxes2[:, 1])  # (N, M)\n",
    "    \n",
    "    # Get bottom right x2,y2 coordinate\n",
    "    x_right = torch.min(boxes1[:, None, 2], boxes2[:, 2])  # (N, M)\n",
    "    y_bottom = torch.min(boxes1[:, None, 3], boxes2[:, 3])  # (N, M)\n",
    "    \n",
    "    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)  # (N, M)\n",
    "    union = area1[:, None] + area2 - intersection_area  # (N, M)\n",
    "    iou = intersection_area / union  # (N, M)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def boxes_to_transformation_targets(ground_truth_boxes, anchors_or_proposals):\n",
    "    r\"\"\"\n",
    "    Given all anchor boxes or proposals in image and their respective\n",
    "    ground truth assignments, we use the x1,y1,x2,y2 coordinates of them\n",
    "    to get tx,ty,tw,th transformation targets for all anchor boxes or proposals\n",
    "    :param ground_truth_boxes: (anchors_or_proposals_in_image, 4)\n",
    "        Ground truth box assignments for the anchors/proposals\n",
    "    :param anchors_or_proposals: (anchors_or_proposals_in_image, 4) Anchors/Proposal boxes\n",
    "    :return: regression_targets: (anchors_or_proposals_in_image, 4) transformation targets tx,ty,tw,th\n",
    "        for all anchors/proposal boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for anchors\n",
    "    widths = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    heights = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * widths\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * heights\n",
    "    \n",
    "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for gt boxes\n",
    "    gt_widths = ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n",
    "    gt_heights = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n",
    "    gt_center_x = ground_truth_boxes[:, 0] + 0.5 * gt_widths\n",
    "    gt_center_y = ground_truth_boxes[:, 1] + 0.5 * gt_heights\n",
    "    \n",
    "    targets_dx = (gt_center_x - center_x) / widths\n",
    "    targets_dy = (gt_center_y - center_y) / heights\n",
    "    targets_dw = torch.log(gt_widths / widths)\n",
    "    targets_dh = torch.log(gt_heights / heights)\n",
    "    regression_targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n",
    "    return regression_targets\n",
    "\n",
    "\n",
    "def apply_regression_pred_to_anchors_or_proposals(box_transform_pred, anchors_or_proposals):\n",
    "    r\"\"\"\n",
    "    Given the transformation parameter predictions for all\n",
    "    input anchors or proposals, transform them accordingly\n",
    "    to generate predicted proposals or predicted boxes\n",
    "    :param box_transform_pred: (num_anchors_or_proposals, num_classes, 4)\n",
    "    :param anchors_or_proposals: (num_anchors_or_proposals, 4)\n",
    "    :return pred_boxes: (num_anchors_or_proposals, num_classes, 4)\n",
    "    \"\"\"\n",
    "    box_transform_pred = box_transform_pred.reshape(\n",
    "        box_transform_pred.size(0), -1, 4)\n",
    "    \n",
    "    # Get cx, cy, w, h from x1,y1,x2,y2\n",
    "    w = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    h = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * w\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * h\n",
    "    \n",
    "    dx = box_transform_pred[..., 0]\n",
    "    dy = box_transform_pred[..., 1]\n",
    "    dw = box_transform_pred[..., 2]\n",
    "    dh = box_transform_pred[..., 3]\n",
    "    # dh -> (num_anchors_or_proposals, num_classes)\n",
    "    \n",
    "    # Prevent sending too large values into torch.exp()\n",
    "    dw = torch.clamp(dw, max=math.log(1000.0 / 16))\n",
    "    dh = torch.clamp(dh, max=math.log(1000.0 / 16))\n",
    "    \n",
    "    pred_center_x = dx * w[:, None] + center_x[:, None]\n",
    "    pred_center_y = dy * h[:, None] + center_y[:, None]\n",
    "    pred_w = torch.exp(dw) * w[:, None]\n",
    "    pred_h = torch.exp(dh) * h[:, None]\n",
    "    # pred_center_x -> (num_anchors_or_proposals, num_classes)\n",
    "    \n",
    "    pred_box_x1 = pred_center_x - 0.5 * pred_w\n",
    "    pred_box_y1 = pred_center_y - 0.5 * pred_h\n",
    "    pred_box_x2 = pred_center_x + 0.5 * pred_w\n",
    "    pred_box_y2 = pred_center_y + 0.5 * pred_h\n",
    "    \n",
    "    pred_boxes = torch.stack((\n",
    "        pred_box_x1,\n",
    "        pred_box_y1,\n",
    "        pred_box_x2,\n",
    "        pred_box_y2),\n",
    "        dim=2)\n",
    "    # pred_boxes -> (num_anchors_or_proposals, num_classes, 4)\n",
    "    return pred_boxes\n",
    "\n",
    "\n",
    "def sample_positive_negative(labels, positive_count, total_count):\n",
    "    # Sample positive and negative proposals\n",
    "    positive = torch.where(labels >= 1)[0]\n",
    "    negative = torch.where(labels == 0)[0]\n",
    "    num_pos = positive_count\n",
    "    num_pos = min(positive.numel(), num_pos)\n",
    "    num_neg = total_count - num_pos\n",
    "    num_neg = min(negative.numel(), num_neg)\n",
    "    perm_positive_idxs = torch.randperm(positive.numel(),\n",
    "                                        device=positive.device)[:num_pos]\n",
    "    perm_negative_idxs = torch.randperm(negative.numel(),\n",
    "                                        device=negative.device)[:num_neg]\n",
    "    pos_idxs = positive[perm_positive_idxs]\n",
    "    neg_idxs = negative[perm_negative_idxs]\n",
    "    sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_pos_idx_mask[pos_idxs] = True\n",
    "    sampled_neg_idx_mask[neg_idxs] = True\n",
    "    return sampled_neg_idx_mask, sampled_pos_idx_mask\n",
    "\n",
    "\n",
    "def clamp_boxes_to_image_boundary(boxes, image_shape):\n",
    "    boxes_x1 = boxes[..., 0]\n",
    "    boxes_y1 = boxes[..., 1]\n",
    "    boxes_x2 = boxes[..., 2]\n",
    "    boxes_y2 = boxes[..., 3]\n",
    "    height, width = image_shape[-2:]\n",
    "    boxes_x1 = boxes_x1.clamp(min=0, max=width)\n",
    "    boxes_x2 = boxes_x2.clamp(min=0, max=width)\n",
    "    boxes_y1 = boxes_y1.clamp(min=0, max=height)\n",
    "    boxes_y2 = boxes_y2.clamp(min=0, max=height)\n",
    "    boxes = torch.cat((\n",
    "        boxes_x1[..., None],\n",
    "        boxes_y1[..., None],\n",
    "        boxes_x2[..., None],\n",
    "        boxes_y2[..., None]),\n",
    "        dim=-1)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def transform_boxes_to_original_size(boxes, new_size, original_size):\n",
    "    r\"\"\"\n",
    "    Boxes are for resized image (min_size=600, max_size=1000).\n",
    "    This method converts the boxes to whatever dimensions\n",
    "    the image was before resizing\n",
    "    :param boxes:\n",
    "    :param new_size:\n",
    "    :param original_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ratios = [\n",
    "        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
    "        / torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
    "        for s, s_orig in zip(new_size, original_size)\n",
    "    ]\n",
    "    ratio_height, ratio_width = ratios\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    xmin = xmin * ratio_width\n",
    "    xmax = xmax * ratio_width\n",
    "    ymin = ymin * ratio_height\n",
    "    ymax = ymax * ratio_height\n",
    "    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "\n",
    "\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    r\"\"\"\n",
    "    RPN with following layers on the feature map\n",
    "        1. 3x3 conv layer followed by Relu\n",
    "        2. 1x1 classification conv with num_anchors(num_scales x num_aspect_ratios) output channels\n",
    "        3. 1x1 classification conv with 4 x num_anchors output channels\n",
    "\n",
    "    Classification is done via one value indicating probability of foreground\n",
    "    with sigmoid applied during inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, scales, aspect_ratios, model_config):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.scales = scales\n",
    "        self.low_iou_threshold = model_config['rpn_bg_threshold']\n",
    "        self.high_iou_threshold = model_config['rpn_fg_threshold']\n",
    "        self.rpn_nms_threshold = model_config['rpn_nms_threshold']\n",
    "        self.rpn_batch_size = model_config['rpn_batch_size']\n",
    "        self.rpn_pos_count = int(model_config['rpn_pos_fraction'] * self.rpn_batch_size)\n",
    "        self.rpn_topk = model_config['rpn_train_topk'] if self.training else model_config['rpn_test_topk']\n",
    "        self.rpn_prenms_topk = model_config['rpn_train_prenms_topk'] if self.training \\\n",
    "            else model_config['rpn_test_prenms_topk']\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
    "        \n",
    "        # 3x3 conv layer\n",
    "        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 1x1 classification conv layer\n",
    "        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n",
    "        \n",
    "        # 1x1 regression\n",
    "        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
    "        \n",
    "        for layer in [self.rpn_conv, self.cls_layer, self.bbox_reg_layer]:\n",
    "            torch.nn.init.normal_(layer.weight, std=0.01)\n",
    "            torch.nn.init.constant_(layer.bias, 0)\n",
    "    \n",
    "    def generate_anchors(self, image, feat):\n",
    "        r\"\"\"\n",
    "        Method to generate anchors. First we generate one set of zero-centred anchors\n",
    "        using the scales and aspect ratios provided.\n",
    "        We then generate shift values in x,y axis for all featuremap locations.\n",
    "        The single zero centred anchors generated are replicated and shifted accordingly\n",
    "        to generate anchors for all feature map locations.\n",
    "        Note that these anchors are generated such that their centre is top left corner of the\n",
    "        feature map cell rather than the centre of the feature map cell.\n",
    "        :param image: (N, C, H, W) tensor\n",
    "        :param feat: (N, C_feat, H_feat, W_feat) tensor\n",
    "        :return: anchor boxes of shape (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        \"\"\"\n",
    "        grid_h, grid_w = feat.shape[-2:]\n",
    "        image_h, image_w = image.shape[-2:]\n",
    "        \n",
    "        # For the vgg16 case stride would be 16 for both h and w\n",
    "        stride_h = torch.tensor(image_h // grid_h, dtype=torch.int64, device=feat.device)\n",
    "        stride_w = torch.tensor(image_w // grid_w, dtype=torch.int64, device=feat.device)\n",
    "        \n",
    "        scales = torch.as_tensor(self.scales, dtype=feat.dtype, device=feat.device)\n",
    "        aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n",
    "        \n",
    "        # Assuming anchors of scale 128 sq pixels\n",
    "        # For 1:1 it would be (128, 128) -> area=16384\n",
    "        # For 2:1 it would be (181.02, 90.51) -> area=16384\n",
    "        # For 1:2 it would be (90.51, 181.02) -> area=16384\n",
    "        \n",
    "        # The below code ensures h/w = aspect_ratios and h*w=1\n",
    "        h_ratios = torch.sqrt(aspect_ratios)\n",
    "        w_ratios = 1 / h_ratios\n",
    "        \n",
    "        # Now we will just multiply h and w with scale(example 128)\n",
    "        # to make h*w = 128 sq pixels and h/w = aspect_ratios\n",
    "        # This gives us the widths and heights of all anchors\n",
    "        # which we need to replicate at all locations\n",
    "        ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        \n",
    "        # Now we make all anchors zero centred\n",
    "        # So x1, y1, x2, y2 = -w/2, -h/2, w/2, h/2\n",
    "        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "        base_anchors = base_anchors.round()\n",
    "        \n",
    "        # Get the shifts in x axis (0, 1,..., W_feat-1) * stride_w\n",
    "        shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n",
    "\n",
    "        # Get the shifts in x axis (0, 1,..., H_feat-1) * stride_h\n",
    "        shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n",
    "        \n",
    "        # Create a grid using these shifts\n",
    "        shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n",
    "        # shifts_x -> (H_feat, W_feat)\n",
    "        # shifts_y -> (H_feat, W_feat)\n",
    "        \n",
    "        shifts_x = shifts_x.reshape(-1)\n",
    "        shifts_y = shifts_y.reshape(-1)\n",
    "        # Setting shifts for x1 and x2(same as shifts_x) and y1 and y2(same as shifts_y)\n",
    "        shifts = torch.stack((shifts_x, shifts_y, shifts_x, shifts_y), dim=1)\n",
    "        # shifts -> (H_feat * W_feat, 4)\n",
    "        \n",
    "        # base_anchors -> (num_anchors_per_location, 4)\n",
    "        # shifts -> (H_feat * W_feat, 4)\n",
    "        # Add these shifts to each of the base anchors\n",
    "        anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4))\n",
    "        # anchors -> (H_feat * W_feat, num_anchors_per_location, 4)\n",
    "        anchors = anchors.reshape(-1, 4)\n",
    "        # anchors -> (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        return anchors\n",
    "    \n",
    "    def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
    "        r\"\"\"\n",
    "        For each anchor assign a ground truth box based on the IOU.\n",
    "        Also creates classification labels to be used for training\n",
    "        label=1 for anchors where maximum IOU with a gtbox > high_iou_threshold\n",
    "        label=0 for anchors where maximum IOU with a gtbox < low_iou_threshold\n",
    "        label=-1 for anchors where maximum IOU with a gtbox between (low_iou_threshold, high_iou_threshold)\n",
    "        :param anchors: (num_anchors_in_image, 4) all anchor boxes\n",
    "        :param gt_boxes: (num_gt_boxes_in_image, 4) all ground truth boxes\n",
    "        :return:\n",
    "            label: (num_anchors_in_image) {-1/0/1}\n",
    "            matched_gt_boxes: (num_anchors_in_image, 4) coordinates of assigned gt_box to each anchor\n",
    "                Even background/to_be_ignored anchors will be assigned some ground truth box.\n",
    "                It's fine, we will use label to differentiate those instances later\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get (gt_boxes, num_anchors_in_image) IOU matrix\n",
    "        iou_matrix = get_iou(gt_boxes, anchors)\n",
    "        \n",
    "        # For each anchor get the gt box index with maximum overlap\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        # best_match_gt_idx -> (num_anchors_in_image)\n",
    "        \n",
    "        # This copy of best_match_gt_idx will be needed later to\n",
    "        # add low quality matches\n",
    "        best_match_gt_idx_pre_thresholding = best_match_gt_idx.clone()\n",
    "        \n",
    "        # Based on threshold, update the values of best_match_gt_idx\n",
    "        # For anchors with highest IOU < low_threshold update to be -1\n",
    "        # For anchors with highest IOU between low_threshold & high threshold update to be -2\n",
    "        below_low_threshold = best_match_iou < self.low_iou_threshold\n",
    "        between_thresholds = (best_match_iou >= self.low_iou_threshold) & (best_match_iou < self.high_iou_threshold)\n",
    "        best_match_gt_idx[below_low_threshold] = -1\n",
    "        best_match_gt_idx[between_thresholds] = -2\n",
    "        \n",
    "        # Add low quality anchor boxes, if for a given ground truth box, these are the ones\n",
    "        # that have highest IOU with that gt box\n",
    "        \n",
    "        # For each gt box, get the maximum IOU value amongst all anchors\n",
    "        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
    "        # best_anchor_iou_for_gt -> (num_gt_boxes_in_image)\n",
    "        \n",
    "        # For each gt box get those anchors\n",
    "        # which have this same IOU as present in best_anchor_iou_for_gt\n",
    "        # This is to ensure if 10 anchors all have the same IOU value,\n",
    "        # which is equal to the highest IOU that this gt box has with any anchor\n",
    "        # then we get all these 10 anchors\n",
    "        gt_pred_pair_with_highest_iou = torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
    "        # gt_pred_pair_with_highest_iou -> [0, 0, 0, 1, 1, 1], [8896,  8905,  8914, 10472, 10805, 11138]\n",
    "        # This means that anchors at the first 3 indexes have an IOU with gt box at index 0\n",
    "        # which is equal to the highest IOU that this gt box has with ANY anchor\n",
    "        # Similarly anchor at last three indexes(10472, 10805, 11138) have an IOU with gt box at index 1\n",
    "        # which is equal to the highest IOU that this gt box has with ANY anchor\n",
    "        # These 6 anchor indexes will also be added as positive anchors\n",
    "        \n",
    "        # Get all the anchors indexes to update\n",
    "        pred_inds_to_update = gt_pred_pair_with_highest_iou[1]\n",
    "        \n",
    "        # Update the matched gt index for all these anchors with whatever was the best gt box\n",
    "        # prior to thresholding\n",
    "        best_match_gt_idx[pred_inds_to_update] = best_match_gt_idx_pre_thresholding[pred_inds_to_update]\n",
    "        \n",
    "        # best_match_gt_idx is either a valid index for all anchors or -1(background) or -2(to be ignored)\n",
    "        # Clamp this so that the best_match_gt_idx is a valid non-negative index\n",
    "        # At this moment the -1 and -2 labelled anchors will be mapped to the 0th gt box\n",
    "        matched_gt_boxes = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "        \n",
    "        # Set all foreground anchor labels as 1\n",
    "        labels = best_match_gt_idx >= 0\n",
    "        labels = labels.to(dtype=torch.float32)\n",
    "        \n",
    "        # Set all background anchor labels as 0\n",
    "        background_anchors = best_match_gt_idx == -1\n",
    "        labels[background_anchors] = 0.0\n",
    "        \n",
    "        # Set all to be ignored anchor labels as -1\n",
    "        ignored_anchors = best_match_gt_idx == -2\n",
    "        labels[ignored_anchors] = -1.0\n",
    "        # Later for classification we will only pick labels which have > 0 label\n",
    "        \n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def filter_proposals(self, proposals, cls_scores, image_shape):\n",
    "        r\"\"\"\n",
    "        This method does three kinds of filtering/modifications\n",
    "        1. Pre NMS topK filtering\n",
    "        2. Make proposals valid by clamping coordinates(0, width/height)\n",
    "        2. Small Boxes filtering based on width and height\n",
    "        3. NMS\n",
    "        4. Post NMS topK filtering\n",
    "        :param proposals: (num_anchors_in_image, 4)\n",
    "        :param cls_scores: (num_anchors_in_image, 4) these are cls logits\n",
    "        :param image_shape: resized image shape needed to clip proposals to image boundary\n",
    "        :return: proposals and cls_scores: (num_filtered_proposals, 4) and (num_filtered_proposals)\n",
    "        \"\"\"\n",
    "        # Pre NMS Filtering\n",
    "        cls_scores = cls_scores.reshape(-1)\n",
    "        cls_scores = torch.sigmoid(cls_scores)\n",
    "        _, top_n_idx = cls_scores.topk(min(self.rpn_prenms_topk, len(cls_scores)))\n",
    "        \n",
    "        cls_scores = cls_scores[top_n_idx]\n",
    "        proposals = proposals[top_n_idx]\n",
    "        ##################\n",
    "        \n",
    "        # Clamp boxes to image boundary\n",
    "        proposals = clamp_boxes_to_image_boundary(proposals, image_shape)\n",
    "        ####################\n",
    "        \n",
    "        # Small boxes based on width and height filtering\n",
    "        min_size = 16\n",
    "        ws, hs = proposals[:, 2] - proposals[:, 0], proposals[:, 3] - proposals[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        proposals = proposals[keep]\n",
    "        cls_scores = cls_scores[keep]\n",
    "        ####################\n",
    "        \n",
    "        # NMS based on objectness scores\n",
    "        keep_mask = torch.zeros_like(cls_scores, dtype=torch.bool)\n",
    "        keep_indices = torch.ops.torchvision.nms(proposals, cls_scores, self.rpn_nms_threshold)\n",
    "        keep_mask[keep_indices] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        # Sort by objectness\n",
    "        post_nms_keep_indices = keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n",
    "        \n",
    "        # Post NMS topk filtering\n",
    "        proposals, cls_scores = (proposals[post_nms_keep_indices[:self.rpn_topk]],\n",
    "                                 cls_scores[post_nms_keep_indices[:self.rpn_topk]])\n",
    "        \n",
    "        return proposals, cls_scores\n",
    "    \n",
    "    def forward(self, image, feat, target=None):\n",
    "        r\"\"\"\n",
    "        Main method for RPN does the following:\n",
    "        1. Call RPN specific conv layers to generate classification and\n",
    "            bbox transformation predictions for anchors\n",
    "        2. Generate anchors for entire image\n",
    "        3. Transform generated anchors based on predicted bbox transformation to generate proposals\n",
    "        4. Filter proposals\n",
    "        5. For training additionally we do the following:\n",
    "            a. Assign target ground truth labels and boxes to each anchors\n",
    "            b. Sample positive and negative anchors\n",
    "            c. Compute classification loss using sampled pos/neg anchors\n",
    "            d. Compute Localization loss using sampled pos anchors\n",
    "        :param image:\n",
    "        :param feat:\n",
    "        :param target:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Call RPN layers\n",
    "        rpn_feat = nn.ReLU()(self.rpn_conv(feat))\n",
    "        cls_scores = self.cls_layer(rpn_feat)\n",
    "        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n",
    "\n",
    "        # Generate anchors\n",
    "        anchors = self.generate_anchors(image, feat)\n",
    "        \n",
    "        # Reshape classification scores to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 1)\n",
    "        # cls_score -> (Batch_Size, Number of Anchors per location, H_feat, W_feat)\n",
    "        number_of_anchors_per_location = cls_scores.size(1)\n",
    "        cls_scores = cls_scores.permute(0, 2, 3, 1)\n",
    "        cls_scores = cls_scores.reshape(-1, 1)\n",
    "        # cls_score -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 1)\n",
    "        \n",
    "        # Reshape bbox predictions to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 4)\n",
    "        # box_transform_pred -> (Batch_Size, Number of Anchors per location*4, H_feat, W_feat)\n",
    "        box_transform_pred = box_transform_pred.view(\n",
    "            box_transform_pred.size(0),\n",
    "            number_of_anchors_per_location,\n",
    "            4,\n",
    "            rpn_feat.shape[-2],\n",
    "            rpn_feat.shape[-1])\n",
    "        box_transform_pred = box_transform_pred.permute(0, 3, 4, 1, 2)\n",
    "        box_transform_pred = box_transform_pred.reshape(-1, 4)\n",
    "        # box_transform_pred -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 4)\n",
    "        \n",
    "        # Transform generated anchors according to box transformation prediction\n",
    "        proposals = apply_regression_pred_to_anchors_or_proposals(\n",
    "            box_transform_pred.detach().reshape(-1, 1, 4),\n",
    "            anchors)\n",
    "        proposals = proposals.reshape(proposals.size(0), 4)\n",
    "        ######################\n",
    "        \n",
    "        proposals, scores = self.filter_proposals(proposals, cls_scores.detach(), image.shape)\n",
    "        rpn_output = {\n",
    "            'proposals': proposals,\n",
    "            'scores': scores\n",
    "        }\n",
    "        if not self.training or target is None:\n",
    "            # If we are not training no need to do anything\n",
    "            return rpn_output\n",
    "        else:\n",
    "            # Assign gt box and label for each anchor\n",
    "            labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_targets_to_anchors(\n",
    "                anchors,\n",
    "                target['bboxes'][0])\n",
    "            \n",
    "            # Based on gt assignment above, get regression target for the anchors\n",
    "            # matched_gt_boxes_for_anchors -> (Number of anchors in image, 4)\n",
    "            # anchors -> (Number of anchors in image, 4)\n",
    "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_anchors, anchors)\n",
    "            \n",
    "            ####### Sampling positive and negative anchors ####\n",
    "            # Our labels were {fg:1, bg:0, to_be_ignored:-1}\n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n",
    "                labels_for_anchors,\n",
    "                positive_count=self.rpn_pos_count,\n",
    "                total_count=self.rpn_batch_size)\n",
    "            \n",
    "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "            \n",
    "            localization_loss = (\n",
    "                    torch.nn.functional.smooth_l1_loss(\n",
    "                        box_transform_pred[sampled_pos_idx_mask],\n",
    "                        regression_targets[sampled_pos_idx_mask],\n",
    "                        beta=1 / 9,\n",
    "                        reduction=\"sum\",\n",
    "                    )\n",
    "                    / (sampled_idxs.numel())\n",
    "            ) \n",
    "\n",
    "            cls_loss = torch.nn.functional.binary_cross_entropy_with_logits(cls_scores[sampled_idxs].flatten(),\n",
    "                                                                            labels_for_anchors[sampled_idxs].flatten())\n",
    "\n",
    "            rpn_output['rpn_classification_loss'] = cls_loss\n",
    "            rpn_output['rpn_localization_loss'] = localization_loss\n",
    "            return rpn_output\n",
    "\n",
    "\n",
    "class ROIHead(nn.Module):\n",
    "    r\"\"\"\n",
    "    ROI head on top of ROI pooling layer for generating\n",
    "    classification and box transformation predictions\n",
    "    We have two fc layers followed by a classification fc layer\n",
    "    and a bbox regression fc layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_config, num_classes, in_channels):\n",
    "        super(ROIHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.roi_batch_size = model_config['roi_batch_size']\n",
    "        self.roi_pos_count = int(model_config['roi_pos_fraction'] * self.roi_batch_size)\n",
    "        self.iou_threshold = model_config['roi_iou_threshold']\n",
    "        self.low_bg_iou = model_config['roi_low_bg_iou']\n",
    "        self.nms_threshold = model_config['roi_nms_threshold']\n",
    "        self.topK_detections = model_config['roi_topk_detections']\n",
    "        self.low_score_threshold = model_config['roi_score_threshold']\n",
    "        self.pool_size = model_config['roi_pool_size']\n",
    "        self.fc_inner_dim = model_config['fc_inner_dim']\n",
    "        \n",
    "        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n",
    "        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
    "        self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n",
    "        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n",
    "        \n",
    "        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
    "        torch.nn.init.constant_(self.cls_layer.bias, 0)\n",
    "\n",
    "        torch.nn.init.normal_(self.bbox_reg_layer.weight, std=0.001)\n",
    "        torch.nn.init.constant_(self.bbox_reg_layer.bias, 0)\n",
    "    \n",
    "    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
    "        r\"\"\"\n",
    "        Given a set of proposals and ground truth boxes and their respective labels.\n",
    "        Use IOU to assign these proposals to some gt box or background\n",
    "        :param proposals: (number_of_proposals, 4)\n",
    "        :param gt_boxes: (number_of_gt_boxes, 4)\n",
    "        :param gt_labels: (number_of_gt_boxes)\n",
    "        :return:\n",
    "            labels: (number_of_proposals)\n",
    "            matched_gt_boxes: (number_of_proposals, 4)\n",
    "        \"\"\"\n",
    "        # Get IOU Matrix between gt boxes and proposals\n",
    "        iou_matrix = get_iou(gt_boxes, proposals)\n",
    "        # For each gt box proposal find best matching gt box\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        background_proposals = (best_match_iou < self.iou_threshold) & (best_match_iou >= self.low_bg_iou)\n",
    "        ignored_proposals = best_match_iou < self.low_bg_iou\n",
    "        \n",
    "        # Update best match of low IOU proposals to -1\n",
    "        best_match_gt_idx[background_proposals] = -1\n",
    "        best_match_gt_idx[ignored_proposals] = -2\n",
    "        \n",
    "        # Get best marching gt boxes for ALL proposals\n",
    "        # Even background proposals would have a gt box assigned to it\n",
    "        # Label will be used to ignore them later\n",
    "        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "        \n",
    "        # Get class label for all proposals according to matching gt boxes\n",
    "        labels = gt_labels[best_match_gt_idx.clamp(min=0)]\n",
    "        labels = labels.to(dtype=torch.int64)\n",
    "        \n",
    "        # Update background proposals to be of label 0(background)\n",
    "        labels[background_proposals] = 0\n",
    "        \n",
    "        # Set all to be ignored anchor labels as -1(will be ignored)\n",
    "        labels[ignored_proposals] = -1\n",
    "        \n",
    "        return labels, matched_gt_boxes_for_proposals\n",
    "    \n",
    "    def forward(self, feat, proposals, image_shape, target):\n",
    "        r\"\"\"\n",
    "        Main method for ROI head that does the following:\n",
    "        1. If training assign target boxes and labels to all proposals\n",
    "        2. If training sample positive and negative proposals\n",
    "        3. If training get bbox transformation targets for all proposals based on assignments\n",
    "        4. Get ROI Pooled features for all proposals\n",
    "        5. Call fc6, fc7 and classification and bbox transformation fc layers\n",
    "        6. Compute classification and localization loss\n",
    "\n",
    "        :param feat:\n",
    "        :param proposals:\n",
    "        :param image_shape:\n",
    "        :param target:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.training and target is not None:\n",
    "            # Add ground truth to proposals\n",
    "            proposals = torch.cat([proposals, target['bboxes'][0]], dim=0)\n",
    "            \n",
    "            gt_boxes = target['bboxes'][0]\n",
    "            gt_labels = target['labels'][0]\n",
    "            \n",
    "            labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(proposals, gt_boxes, gt_labels)\n",
    "            \n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(labels,\n",
    "                                                                                  positive_count=self.roi_pos_count,\n",
    "                                                                                  total_count=self.roi_batch_size)\n",
    "            \n",
    "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "            \n",
    "            # Keep only sampled proposals\n",
    "            proposals = proposals[sampled_idxs]\n",
    "            labels = labels[sampled_idxs]\n",
    "            matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n",
    "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_proposals, proposals)\n",
    "            # regression_targets -> (sampled_training_proposals, 4)\n",
    "            # matched_gt_boxes_for_proposals -> (sampled_training_proposals, 4)\n",
    "        \n",
    "        # Get desired scale to pass to roi pooling function\n",
    "        # For vgg16 case this would be 1/16 (0.0625)\n",
    "        size = feat.shape[-2:]\n",
    "        possible_scales = []\n",
    "        for s1, s2 in zip(size, image_shape):\n",
    "            approx_scale = float(s1) / float(s2)\n",
    "            scale = 2 ** float(torch.tensor(approx_scale).log2().round())\n",
    "            possible_scales.append(scale)\n",
    "        assert possible_scales[0] == possible_scales[1]\n",
    "        \n",
    "        # ROI pooling and call all layers for prediction\n",
    "        proposal_roi_pool_feats = torchvision.ops.roi_pool(feat, [proposals],\n",
    "                                                           output_size=self.pool_size,\n",
    "                                                           spatial_scale=possible_scales[0])\n",
    "        proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n",
    "        box_fc_6 = torch.nn.functional.relu(self.fc6(proposal_roi_pool_feats))\n",
    "        box_fc_7 = torch.nn.functional.relu(self.fc7(box_fc_6))\n",
    "        cls_scores = self.cls_layer(box_fc_7)\n",
    "        box_transform_pred = self.bbox_reg_layer(box_fc_7)\n",
    "        # cls_scores -> (proposals, num_classes)\n",
    "        # box_transform_pred -> (proposals, num_classes * 4)\n",
    "        ##############################################\n",
    "        \n",
    "        num_boxes, num_classes = cls_scores.shape\n",
    "        box_transform_pred = box_transform_pred.reshape(num_boxes, num_classes, 4)\n",
    "        frcnn_output = {}\n",
    "        if self.training and target is not None:\n",
    "            classification_loss = torch.nn.functional.cross_entropy(cls_scores, labels)\n",
    "            \n",
    "            # Compute localization loss only for non-background labelled proposals\n",
    "            fg_proposals_idxs = torch.where(labels > 0)[0]\n",
    "            # Get class labels for these positive proposals\n",
    "            fg_cls_labels = labels[fg_proposals_idxs]\n",
    "            \n",
    "            localization_loss = torch.nn.functional.smooth_l1_loss(\n",
    "                box_transform_pred[fg_proposals_idxs, fg_cls_labels],\n",
    "                regression_targets[fg_proposals_idxs],\n",
    "                beta=1/9,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "            localization_loss = localization_loss / labels.numel()\n",
    "            frcnn_output['frcnn_classification_loss'] = classification_loss\n",
    "            frcnn_output['frcnn_localization_loss'] = localization_loss\n",
    "        \n",
    "        if self.training:\n",
    "            return frcnn_output\n",
    "        else:\n",
    "            device = cls_scores.device\n",
    "            # Apply transformation predictions to proposals\n",
    "            pred_boxes = apply_regression_pred_to_anchors_or_proposals(box_transform_pred, proposals)\n",
    "            pred_scores = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
    "            \n",
    "            # Clamp box to image boundary\n",
    "            pred_boxes = clamp_boxes_to_image_boundary(pred_boxes, image_shape)\n",
    "            \n",
    "            # create labels for each prediction\n",
    "            pred_labels = torch.arange(num_classes, device=device)\n",
    "            pred_labels = pred_labels.view(1, -1).expand_as(pred_scores)\n",
    "            \n",
    "            # remove predictions with the background label\n",
    "            pred_boxes = pred_boxes[:, 1:]\n",
    "            pred_scores = pred_scores[:, 1:]\n",
    "            pred_labels = pred_labels[:, 1:]\n",
    "            \n",
    "            # pred_boxes -> (number_proposals, num_classes-1, 4)\n",
    "            # pred_scores -> (number_proposals, num_classes-1)\n",
    "            # pred_labels -> (number_proposals, num_classes-1)\n",
    "            \n",
    "            # batch everything, by making every class prediction be a separate instance\n",
    "            pred_boxes = pred_boxes.reshape(-1, 4)\n",
    "            pred_scores = pred_scores.reshape(-1)\n",
    "            pred_labels = pred_labels.reshape(-1)\n",
    "            \n",
    "            pred_boxes, pred_labels, pred_scores = self.filter_predictions(pred_boxes, pred_labels, pred_scores)\n",
    "            frcnn_output['boxes'] = pred_boxes\n",
    "            frcnn_output['scores'] = pred_scores\n",
    "            frcnn_output['labels'] = pred_labels\n",
    "            return frcnn_output\n",
    "    \n",
    "    def filter_predictions(self, pred_boxes, pred_labels, pred_scores):\n",
    "        r\"\"\"\n",
    "        Method to filter predictions by applying the following in order:\n",
    "        1. Filter low scoring boxes\n",
    "        2. Remove small size boxes∂\n",
    "        3. NMS for each class separately\n",
    "        4. Keep only topK detections\n",
    "        :param pred_boxes:\n",
    "        :param pred_labels:\n",
    "        :param pred_scores:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # remove low scoring boxes\n",
    "        keep = torch.where(pred_scores > self.low_score_threshold)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        # Remove small boxes\n",
    "        min_size = 16\n",
    "        ws, hs = pred_boxes[:, 2] - pred_boxes[:, 0], pred_boxes[:, 3] - pred_boxes[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        # Class wise nms\n",
    "        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n",
    "        for class_id in torch.unique(pred_labels):\n",
    "            curr_indices = torch.where(pred_labels == class_id)[0]\n",
    "            curr_keep_indices = torch.ops.torchvision.nms(pred_boxes[curr_indices],\n",
    "                                                          pred_scores[curr_indices],\n",
    "                                                          self.nms_threshold)\n",
    "            keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        post_nms_keep_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n",
    "        keep = post_nms_keep_indices[:self.topK_detections]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        return pred_boxes, pred_labels, pred_scores\n",
    "\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, model_config, num_classes):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.model_config = model_config\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "        self.backbone = vgg16.features[:-1]\n",
    "        self.rpn = RegionProposalNetwork(model_config['backbone_out_channels'],\n",
    "                                         scales=model_config['scales'],\n",
    "                                         aspect_ratios=model_config['aspect_ratios'],\n",
    "                                         model_config=model_config)\n",
    "        self.roi_head = ROIHead(model_config, num_classes, in_channels=model_config['backbone_out_channels'])\n",
    "        for layer in self.backbone[:10]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.image_mean = [0.485, 0.456, 0.406]\n",
    "        self.image_std = [0.229, 0.224, 0.225]\n",
    "        self.min_size = model_config['min_im_size']\n",
    "        self.max_size = model_config['max_im_size']\n",
    "    \n",
    "    def normalize_resize_image_and_boxes(self, image, bboxes):\n",
    "        dtype, device = image.dtype, image.device\n",
    "        \n",
    "        # Normalize\n",
    "        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device)\n",
    "        std = torch.as_tensor(self.image_std, dtype=dtype, device=device)\n",
    "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "        #############\n",
    "        \n",
    "        # Resize to 1000x600 such that lowest size dimension is scaled upto 600\n",
    "        # but larger dimension is not more than 1000\n",
    "        # So compute scale factor for both and scale is minimum of these two\n",
    "        h, w = image.shape[-2:]\n",
    "        im_shape = torch.tensor(image.shape[-2:])\n",
    "        min_size = torch.min(im_shape).to(dtype=torch.float32)\n",
    "        max_size = torch.max(im_shape).to(dtype=torch.float32)\n",
    "        scale = torch.min(float(self.min_size) / min_size, float(self.max_size) / max_size)\n",
    "        scale_factor = scale.item()\n",
    "        \n",
    "        # Resize image based on scale computed\n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image,\n",
    "            size=None,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=\"bilinear\",\n",
    "            recompute_scale_factor=True,\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        if bboxes is not None:\n",
    "            # Resize boxes by\n",
    "            ratios = [\n",
    "                torch.tensor(s, dtype=torch.float32, device=bboxes.device)\n",
    "                / torch.tensor(s_orig, dtype=torch.float32, device=bboxes.device)\n",
    "                for s, s_orig in zip(image.shape[-2:], (h, w))\n",
    "            ]\n",
    "            ratio_height, ratio_width = ratios\n",
    "            xmin, ymin, xmax, ymax = bboxes.unbind(2)\n",
    "            xmin = xmin * ratio_width\n",
    "            xmax = xmax * ratio_width\n",
    "            ymin = ymin * ratio_height\n",
    "            ymax = ymax * ratio_height\n",
    "            bboxes = torch.stack((xmin, ymin, xmax, ymax), dim=2)\n",
    "        return image, bboxes\n",
    "    \n",
    "    def forward(self, image, target=None):\n",
    "        old_shape = image.shape[-2:]\n",
    "        if self.training:\n",
    "            # Normalize and resize boxes\n",
    "            image, bboxes = self.normalize_resize_image_and_boxes(image, target['bboxes'])\n",
    "            target['bboxes'] = bboxes\n",
    "        else:\n",
    "            image, _ = self.normalize_resize_image_and_boxes(image, None)\n",
    "        \n",
    "        # Call backbone\n",
    "        feat = self.backbone(image)\n",
    "        \n",
    "        # Call RPN and get proposals\n",
    "        rpn_output = self.rpn(image, feat, target)\n",
    "        proposals = rpn_output['proposals']\n",
    "        \n",
    "        # Call ROI head and convert proposals to boxes\n",
    "        frcnn_output = self.roi_head(feat, proposals, image.shape[-2:], target)\n",
    "        if not self.training:\n",
    "            # Transform boxes to original image dimensions called only during inference\n",
    "            frcnn_output['boxes'] = transform_boxes_to_original_size(frcnn_output['boxes'],\n",
    "                                                                     image.shape[-2:],\n",
    "                                                                     old_shape)\n",
    "        return rpn_output, frcnn_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorvch_env",
   "language": "python",
   "name": "pytorvch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
